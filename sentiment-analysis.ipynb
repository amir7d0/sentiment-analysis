{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install -qq datasets evaluate transformers\n!pip install -qq wandb","metadata":{"id":"bOUjzSKMk6gP","outputId":"8d5b3354-b265-4f4b-f53a-b6b3a0d1b425","tags":[],"execution":{"iopub.status.busy":"2023-03-11T11:40:55.729780Z","iopub.execute_input":"2023-03-11T11:40:55.730527Z","iopub.status.idle":"2023-03-11T11:41:20.508897Z","shell.execute_reply.started":"2023-03-11T11:40:55.730497Z","shell.execute_reply":"2023-03-11T11:41:20.507456Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import wandb\nfrom kaggle_secrets import UserSecretsClient\n\nuser_secrets = UserSecretsClient()\nWANDB_API_KEY = user_secrets.get_secret(\"wandb_key\")\n\nWANDB_ENTITY='amir7d0'\nWANDB_PROJECT='sentiment-analysis'\n\n\nwandb.login(key=WANDB_API_KEY)","metadata":{"execution":{"iopub.status.busy":"2023-03-13T12:54:03.419899Z","iopub.execute_input":"2023-03-13T12:54:03.420912Z","iopub.status.idle":"2023-03-13T12:54:06.186923Z","shell.execute_reply.started":"2023-03-13T12:54:03.420849Z","shell.execute_reply":"2023-03-13T12:54:06.185814Z"},"trusted":true},"execution_count":20,"outputs":[{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: W&B API key is configured. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n","output_type":"stream"},{"execution_count":20,"output_type":"execute_result","data":{"text/plain":"True"},"metadata":{}}]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 1. Business / Research Understanding Phase","metadata":{"id":"4-LYN1ir4a2B"}},{"cell_type":"markdown","source":"The goal of this sentiment analysis project is to provide insights into the sentiment of customers towards products on Amazon. By analyzing customer reviews, we can identify trends and patterns in customer sentiment, which can be used to improve product development, marketing strategies, and customer service. Specifically, we will be analyzing the Amazon review dataset, which contains millions of reviews of various products. Our analysis will focus on identifying the sentiment of these reviews, categorizing them as positive, negative, or neutral, and examining the factors that contribute to customer sentiment. This information can be valuable to businesses looking to improve their products and customer experience, as well as researchers interested in understanding consumer behavior and sentiment.","metadata":{"id":"XWqP2vsa41i5"}},{"cell_type":"markdown","source":"Amazon product reviews dataset contains reviews in English, Japanese, German, French, Chinese, and Spanish, collected between November 1, 2015, and November 1, 2019. Each record in the dataset contains the review text, the review title, the star rating, an anonymized reviewer ID, an anonymized product ID, and the coarse-grained product category (e.g. ‘books’, ‘appliances’, etc.) The corpus is balanced across stars, so each star rating constitutes 20% of the reviews in each language.\n\nFor each language, there are 200,000, 5,000, and 5,000 reviews in the training, development, and test sets respectively. The maximum number of reviews per reviewer is 20 and the maximum number of reviews per product is 20. All reviews are truncated after 2,000 characters, and all reviews are at least 20 characters long.\n\n* In this project we use English language reviews, so we have 200,000 records for the training, 5000 for the validation, and 5000 for the test set.\n* There are two tables available for EDA: \"eda_table\" which contains the entire dataset of 210,000 records, and \"eda_table_sample\" which is a sample of 10,000 records from the dataset.\n* For this project, we use the \"review_body\" column for input text and the \"stars\" column for labels which is five classes.","metadata":{}},{"cell_type":"markdown","source":"## 2. Data Preparation/Understanding Phase","metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"id":"wkoEQHCu4eoc","tags":[]}},{"cell_type":"code","source":"from datasets import get_dataset_config_names, load_dataset_builder, load_dataset\n\nDATASET_PATH = \"amazon_reviews_multi\"\nDATASET_CONFIG = 'en'\n","metadata":{"id":"Al7ASv5F4XJa"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"configs = get_dataset_config_names(DATASET_PATH)\nprint(configs)","metadata":{"id":"AioiarXwk9ds","outputId":"9b70cdb2-f0f8-467a-b015-a321905f2519"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ds_builder = load_dataset_builder(DATASET_PATH, DATASET_CONFIG)\n\nprint(ds_builder.info.description)\nprint('dataset splits: ', ds_builder.info.splits)\nprint('dataset features:', ds_builder.info.features)","metadata":{"id":"ETPGqfTOvH-U","outputId":"0cfb1b1d-de3b-408a-b45f-6375cfe05de5"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dataset = load_dataset(DATASET_PATH, DATASET_CONFIG)","metadata":{"id":"KaVPfH9EmPZt","outputId":"c8a92396-741e-44ce-9f4b-e46ad41abf10"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dataset","metadata":{"id":"mgJT6HH5oeK5","outputId":"4910de2d-26f4-48ae-adad-ca045afbf07c"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 3. Exploratory Data Analysis Phase","metadata":{"id":"otg7m0Ob4pfc","tags":[]}},{"cell_type":"code","source":"run = wandb.init(project=WANDB_PROJECT, job_type='upload-dataset')\nartifact = wandb.Artifact(name='amazon_reviews_english', type='dataset')\n","metadata":{"id":"XMtTngMq53tg","outputId":"17728ca3-857c-443f-d7b5-d863e2057d3f"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"files = [dataset.cache_files[key][0]['filename'] for key in dataset.cache_files]\n\nfor path in files:\n    artifact.add_file(local_path=path)\n","metadata":{"id":"OOOPzIgH6xtF"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"features = ds_builder.info.features\n\nfor key, value in features.items():\n    print(key, value)","metadata":{"id":"uQNNi7z37RLl","outputId":"5d155183-4534-4acf-bdad-949503e4a743"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\n\ndf = pd.DataFrame()\nfor key in dataset:\n    df_tmp = dataset[key].to_pandas()\n    df_tmp['split'] = key\n    df = pd.concat([df, df_tmp])\n\ndf.shape","metadata":{"id":"C0NAyCL4DciR","outputId":"00dccf69-f00d-4548-c877-43fb2627a506"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_sample = df[df['split'] == 'train'].sample(1000)\n\ntable = wandb.Table(columns=list(df_sample.columns), data=df_sample)\nartifact.add(table, \"eda_table_sample\")","metadata":{"id":"J6oQB0MVKg5Z","outputId":"c266050d-f055-406b-e12e-7b8946a37789"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"table = wandb.Table(columns=list(df.columns), data=df)\nwandb.Table.MAX_ROWS = df.shape[0] + 1000\nartifact.add(table, \"eda_table\")","metadata":{"id":"f4kvQ_RR8kFG","outputId":"412b7087-c87e-4743-8fef-1009a44a5232"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"run.log_artifact(artifact)\nrun.finish()","metadata":{"id":"9LjN6HZq53nx"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"id":"IJblWA9jJuGM"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"id":"QkQ-D4I5JuCl"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 4. Setup Phase","metadata":{"id":"9le_V9j-CxI8","tags":[]}},{"cell_type":"code","source":"from datasets import load_dataset\nfrom transformers import AutoTokenizer, DataCollatorWithPadding\n\nimport numpy as np\nimport pandas as pd\n\nDATASET_PATH = \"amazon_reviews_multi\"\nDATASET_CONFIG = 'en'\nTEXT_COL = 'review_body'\nLABEL_COL = 'stars'\n\nMODEL_NAME = \"distilbert-base-uncased\"\nMAX_LEN = 128\nMODEL_DIR = \"HF_Model\"\n\nBATCH_SIZE = 8\nEPOCHS = 1\nLEARNING_RATE = 5e-5","metadata":{"id":"KgMp7PAC6JSx","tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nraw_datasets = load_dataset(DATASET_PATH, DATASET_CONFIG)\nraw_datasets = raw_datasets.rename_columns({TEXT_COL: 'text', LABEL_COL: 'labels'})\ndrop_columns = list(set(raw_datasets[\"train\"].column_names) - set(['text', 'labels']))\nraw_datasets = raw_datasets.remove_columns(drop_columns)\n\n# find number of classes and map 1-5 stars to a range of 0 to 4\nnumber_of_classes = len(pd.unique(raw_datasets['train']['labels']))\nclass_map = dict(zip(pd.unique(raw_datasets['train']['labels']), \n                     pd.unique(raw_datasets['train']['labels'])-1))\n\nraw_datasets = raw_datasets.map(lambda example: {'labels': class_map[example['labels']]})\nraw_datasets","metadata":{"id":"H2pW0x_tFBQ2","outputId":"9745b8dd-02b5-40ef-a9fb-d24631df0244","tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\ntokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n\ndef tokenization(example):\n    return tokenizer(example['text'], max_length=MAX_LEN, padding=True, truncation=True)\n\ntokenized_datasets = raw_datasets.map(tokenization, batched=True)\ntokenized_datasets = tokenized_datasets.remove_columns(['text'])\ntokenized_datasets[\"train\"].column_names","metadata":{"id":"oClYRqhxXPE7","outputId":"08851caf-2c86-4c54-cc2c-ad2ffde9ce2d","tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### TensorFlow / Keras","metadata":{"id":"8M7W127E1U-f","tags":[]}},{"cell_type":"code","source":"# create tensorflow dataset\ndata_collator = DataCollatorWithPadding(tokenizer=tokenizer, return_tensors=\"tf\")\n\ntf_train_dataset = tokenized_datasets[\"train\"].to_tf_dataset(\n    columns=[\"attention_mask\", \"input_ids\"],\n    label_cols=[\"labels\"],\n    shuffle=True,\n    collate_fn=data_collator,\n    batch_size=BATCH_SIZE,\n)\n\ntf_validation_dataset = tokenized_datasets[\"validation\"].to_tf_dataset(\n    columns=[\"attention_mask\", \"input_ids\"],\n    label_cols=[\"labels\"],\n    shuffle=False,\n    collate_fn=data_collator,\n    batch_size=BATCH_SIZE,\n)\n\ntf_test_dataset = tokenized_datasets[\"test\"].to_tf_dataset(\n    columns=[\"attention_mask\", \"input_ids\"],\n    label_cols=[\"labels\"],\n    shuffle=False,\n    collate_fn=data_collator,\n    batch_size=BATCH_SIZE,\n)","metadata":{"id":"F3UDrD3O1Tux","tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from transformers import TFAutoModelForSequenceClassification\nfrom tensorflow.keras.losses import SparseCategoricalCrossentropy\nfrom tensorflow.keras.optimizers import Adam\nimport tensorflow as tf\n\n# run on multiple GPUs\ngpus = tf.config.list_logical_devices('GPU')\nstrategy = tf.distribute.MirroredStrategy(gpus)\nprint(\"Num GPUs Available: \", strategy.num_replicas_in_sync)\nwith strategy.scope():\n    model = TFAutoModelForSequenceClassification.from_pretrained(MODEL_NAME, \n                                                             num_labels=number_of_classes)\n\n    model.compile(\n        optimizer=Adam(LEARNING_RATE),\n        loss=SparseCategoricalCrossentropy(from_logits=True),\n        metrics=[\"accuracy\"],\n    )","metadata":{"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import wandb\nfrom wandb.keras import WandbMetricsLogger\n\ntrain_config = {\n    \"model\": f\"{MODEL_NAME}-finetuned\", \n    \"batch_size\": BATCH_SIZE,\n    \"epochs\": EPOCHS,\n    \"learning_rate\": LEARNING_RATE,\n    \"pretrained\": True,\n}\n\nrun = wandb.init(project=WANDB_PROJECT, job_type=\"training\", config=train_config)","metadata":{"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Train the model for one epoch\nmodel.fit(\n    tf_train_dataset,\n    validation_data=tf_validation_dataset,\n    epochs=EPOCHS,\n    callbacks=[WandbMetricsLogger(log_freq='batch')]\n)","metadata":{"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"label_names = ['1 star', '2 stars', '3 stars', '4 stars', '5 stars']\n\nmodel.config.id2label = {str(i): label for i, label in enumerate(label_names)}\nmodel.config.label2id = {label: str(i) for i, label in enumerate(label_names)}","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"wandb.finish(quiet=True)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"id":"WBpdl9pFnDTR"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"id":"YZVQA5DYnDQH"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 5. Modeling Phase","metadata":{"id":"NxeM_AyYC2Gq"}},{"cell_type":"code","source":"from datasets import load_dataset\nfrom transformers import AutoTokenizer, DataCollatorWithPadding\n\nimport numpy as np\nimport pandas as pd\n\n\nDATASET_PATH = \"amazon_reviews_multi\"\nDATASET_CONFIG = 'en'\nTEXT_COL = 'review_body'\nLABEL_COL = 'stars'\n\nMODEL_NAME = \"distilbert-base-uncased\"\nMAX_LEN = 128\nMODEL_DIR = \"HF_Model\"\n\nBATCH_SIZE = 8\nEPOCHS = 1\nLEARNING_RATE = 5e-5","metadata":{"execution":{"iopub.status.busy":"2023-03-10T18:17:25.916068Z","iopub.execute_input":"2023-03-10T18:17:25.917022Z","iopub.status.idle":"2023-03-10T18:17:36.877854Z","shell.execute_reply.started":"2023-03-10T18:17:25.916979Z","shell.execute_reply":"2023-03-10T18:17:36.876710Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"raw_datasets = load_dataset(DATASET_PATH, DATASET_CONFIG)\nraw_datasets = raw_datasets.rename_columns({TEXT_COL: 'text', LABEL_COL: 'labels'})\ndrop_columns = list(set(raw_datasets[\"train\"].column_names) - set(['text', 'labels']))\nraw_datasets = raw_datasets.remove_columns(drop_columns)\n\n# find number of classes and map 1-5 stars to a range of 0 to 4\nnumber_of_classes = len(pd.unique(raw_datasets['train']['labels']))\nclass_map = dict(zip(pd.unique(raw_datasets['train']['labels']), \n                     pd.unique(raw_datasets['train']['labels'])-1))\n\nraw_datasets = raw_datasets.map(lambda example: {'labels': class_map[example['labels']]})\n\ntokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\ndef tokenization(example):\n    return tokenizer(example['text'], max_length=MAX_LEN, padding=True, truncation=True)\n\ntokenized_datasets = raw_datasets.map(tokenization, batched=True)\ntokenized_datasets = tokenized_datasets.remove_columns(['text'])\nprint(tokenized_datasets[\"train\"].column_names)","metadata":{"execution":{"iopub.status.busy":"2023-03-10T18:17:43.546453Z","iopub.execute_input":"2023-03-10T18:17:43.548082Z","iopub.status.idle":"2023-03-10T18:18:39.855057Z","shell.execute_reply.started":"2023-03-10T18:17:43.548027Z","shell.execute_reply":"2023-03-10T18:18:39.853750Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_dataset(batch_size=8, columns=[\"attention_mask\", \"input_ids\"]):\n    # create tensorflow dataset\n    data_collator = DataCollatorWithPadding(tokenizer=tokenizer, return_tensors=\"tf\")\n    \n    tf_train_dataset = tokenized_datasets[\"train\"].to_tf_dataset(\n        columns=columns,\n        label_cols=[\"labels\"],\n        shuffle=True,\n        collate_fn=data_collator,\n        batch_size=batch_size,\n    )\n\n    tf_validation_dataset = tokenized_datasets[\"validation\"].to_tf_dataset(\n        columns=columns,\n        label_cols=[\"labels\"],\n        shuffle=False,\n        collate_fn=data_collator,\n        batch_size=batch_size,\n    )\n\n    tf_test_dataset = tokenized_datasets[\"test\"].to_tf_dataset(\n        columns=columns,\n        label_cols=[\"labels\"],\n        shuffle=False,\n        collate_fn=data_collator,\n        batch_size=batch_size,\n    )\n    return tf_train_dataset, tf_validation_dataset, tf_test_dataset","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from transformers import TFAutoModelForSequenceClassification, AutoConfig\nfrom tensorflow.keras.losses import SparseCategoricalCrossentropy\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.optimizers.schedules import PolynomialDecay\nfrom wandb.keras import WandbMetricsLogger, WandbModelCheckpoint\nimport tensorflow as tf\nimport os\n\n\nos.makedirs(MODEL_DIR, exist_ok=True)\nmodel_config_path = f'{MODEL_DIR}/config.json'\nmodel_config = AutoConfig.from_pretrained(MODEL_NAME, num_labels=5)\nlabel_names = ['1 star', '2 stars', '3 stars', '4 stars', '5 stars']\n\nmodel_config.id2label = {str(i): label for i, label in enumerate(label_names)}\nmodel_config.label2id = {label: i for i, label in enumerate(label_names)}\nwith open(model_config_path, 'w') as f:\n    model_config.to_json_file(model_config_path)\n\n\ndef train():\n    default_config = {\n        'dataset_name': DATASET_PATH,\n        'model_name': MODEL_NAME,\n        'max_length': MAX_LEN,\n        'model_dir': MODEL_DIR,\n        'batch_size': BATCH_SIZE,\n        'learning_rate': LEARNING_RATE,\n        'epochs': EPOCHS,\n    }\n    \n    wandb.init(job_type='hyperparam-tuning', config=default_config)\n    wandb.save(model_config_path)\n    config = wandb.config\n    \n    train_dataset, validation_dataset, test_dataset = get_dataset(batch_size=config.batch_size)\n    \n    num_train_steps = len(train_dataset) * config.epochs\n    lr_scheduler = PolynomialDecay(\n        initial_learning_rate=config.learning_rate, end_learning_rate=0.0, \n        decay_steps=num_train_steps\n    )\n\n    # run on multiple GPUs\n    gpus = tf.config.list_logical_devices('GPU')\n    strategy = tf.distribute.MirroredStrategy(gpus)\n    print(\"Num GPUs Available: \", strategy.num_replicas_in_sync)\n    with strategy.scope():\n        model = TFAutoModelForSequenceClassification.from_pretrained(config.model_name, \n                                                                 num_labels=number_of_classes)\n        model.compile(optimizer=Adam(learning_rate=lr_scheduler), \n                      loss=SparseCategoricalCrossentropy(from_logits=True), \n                      metrics=[\"accuracy\"])\n    # Train the model for one epoch\n    model.fit(\n        train_dataset.take(500),\n        validation_data=validation_dataset.take(100),\n        epochs=config.epochs,\n        callbacks=[\n            WandbMetricsLogger(log_freq='batch'), \n            WandbModelCheckpoint(filepath=f'{wandb.run.dir}/{config.model_dir}/tf_model.h5', save_best_only=True, save_weights_only=True),\n        ]\n    )\n    # model.save_pretrained(f'{wandb.run.dir}/HF_Model/')\n    \n    ","metadata":{"id":"xb_aWfe5qCTq","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nsweep_configuration = {\n    \"name\": \"hyperparam-tuning-distilbert\",\n    \"metric\": {\"name\": \"epoch/val_accuracy\", \"goal\": \"maximize\"},\n    \"method\": \"grid\",\n    \"parameters\": {\n        'batch_size': {'values': [8, 32]},\n        'learning_rate': {'values': [5e-5, 1e-4]},\n        'epochs': {'values': [1, 3]},\n    },\n}\n\nsweep_id = wandb.sweep(sweep_configuration, project=WANDB_PROJECT, entity=WANDB_ENTITY)\n\n# run the sweep\nwandb.agent(sweep_id, function=train, count=8)\nwandb.finish(quiet=True)","metadata":{"scrolled":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import wandb\napi = wandb.Api()\n\nsweep = api.sweep(f\"{WANDB_ENTITY}/{WANDB_PROJECT}/{sweep_id}\")\nmetric_name = sweep_configuration['metric']['name']\n\nruns = sorted(sweep.runs, key=lambda run: run.summary.get(metric_name, 0), reverse=True)\nmetric_value = runs[0].summary.get(metric_name, 0)\nprint(f\"Best run {runs[0].name} with {metric_name} = {metric_value}\")\n\nruns[0].file(f'{MODEL_DIR}/model.h5').download(replace=True)\nruns[0].file(f'{MODEL_DIR}/config.json').download(replace=True)\n\nprint(f\"Best Model files downloaded to ./{MODEL_DIR}/\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## convert tf_model.h5 to model.bin\n# from transformers import AutoModelForSequenceClassification, AutoConfig, TFAutoModelForSequenceClassification\n\n# tf_model = TFAutoModelForSequenceClassification.from_pretrained(MODEL_NAME)\n# config = AutoConfig.from_pretrained(MODEL_NAME)\n# pt_model = AutoModelForSequenceClassification.from_config(config)\n# pt_model = load_tf2_weights_in_pytorch_model(pt_model, tf_model.weights)\n# pt_model.push_to_hub(MODEL_NAME)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 6. Evaluation Phase","metadata":{"id":"JsBJ50ikC3qc"}},{"cell_type":"markdown","source":"Fine-tune for 5 epochs with optimal hyperparameters.","metadata":{}},{"cell_type":"code","source":"from transformers import pipeline\nfrom datasets import load_dataset\n\n\nDATASET_PATH = \"amazon_reviews_multi\"\nDATASET_CONFIG = \"en\"\nTEXT_COL = \"review_body\"\nLABEL_COL = \"stars\"\n\nMODEL_NAME = \"amir7d0/distilbert-base-uncased-finetuned-amazon-reviews\"\nMAX_LEN = 128","metadata":{"execution":{"iopub.status.busy":"2023-03-13T19:17:33.704580Z","iopub.execute_input":"2023-03-13T19:17:33.704976Z","iopub.status.idle":"2023-03-13T19:17:36.406661Z","shell.execute_reply.started":"2023-03-13T19:17:33.704939Z","shell.execute_reply":"2023-03-13T19:17:36.405541Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"from transformers import AutoTokenizer\nimport pandas as pd\n\nraw_datasets = load_dataset(DATASET_PATH, DATASET_CONFIG)\nraw_datasets = raw_datasets.rename_columns({TEXT_COL: 'text', LABEL_COL: 'labels'})\ndrop_columns = list(set(raw_datasets[\"train\"].column_names) - set(['text', 'labels']))\nraw_datasets = raw_datasets.remove_columns(drop_columns)\n\n# find number of classes and map 1-5 stars to a range of 0 to 4\nnumber_of_classes = len(pd.unique(raw_datasets['train']['labels']))\nclass_map = dict(zip(pd.unique(raw_datasets['train']['labels']), \n                     pd.unique(raw_datasets['train']['labels'])-1))\n\nraw_datasets = raw_datasets.map(lambda example: {'labels': class_map[example['labels']]})\n\ntokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\ndef tokenization(example):\n    return tokenizer(example['text'], max_length=MAX_LEN, padding=True, truncation=True)\n\ntokenized_datasets = raw_datasets.map(tokenization, batched=True)\ntokenized_datasets = tokenized_datasets.remove_columns(['text'])\nprint(tokenized_datasets[\"train\"].column_names)","metadata":{"execution":{"iopub.status.busy":"2023-03-13T19:17:36.408709Z","iopub.execute_input":"2023-03-13T19:17:36.409605Z","iopub.status.idle":"2023-03-13T19:19:02.431123Z","shell.execute_reply.started":"2023-03-13T19:17:36.409554Z","shell.execute_reply":"2023-03-13T19:19:02.430219Z"},"trusted":true},"execution_count":6,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading builder script:   0%|          | 0.00/2.74k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a45690745bcc48e5a26e40f5eeb3d9bc"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading metadata:   0%|          | 0.00/3.62k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8c2f5aa3d8f54c0f8ac2675fc4ad6f49"}},"metadata":{}},{"name":"stdout","text":"Downloading and preparing dataset amazon_reviews_multi/en (download: 82.11 MiB, generated: 58.69 MiB, post-processed: Unknown size, total: 140.79 MiB) to /root/.cache/huggingface/datasets/amazon_reviews_multi/en/1.0.0/724e94f4b0c6c405ce7e476a6c5ef4f87db30799ad49f765094cf9770e0f7609...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Downloading data files:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3695f2c220c04e6ebb9b43544692fc04"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/82.0M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"61ae504867a54c6fa686a73c0028c758"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Extracting data files:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"47028cdf629e431697cc1b3328b5179a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading data files:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a54ff02d971940cea517d22b1c1a3f63"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/2.06M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fc476949130c4ebeb9fe7cdee99492b4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Extracting data files:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5f36ecda129c4a528c58accf57bd79c0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading data files:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"42cde34c800843aaa4ec37cce318610f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/2.05M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4539afca012b4978a33a2e265b50c77c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Extracting data files:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"26e53ae249f149f896058f499a6c8714"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/200000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating validation split:   0%|          | 0/5000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating test split:   0%|          | 0/5000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Dataset amazon_reviews_multi downloaded and prepared to /root/.cache/huggingface/datasets/amazon_reviews_multi/en/1.0.0/724e94f4b0c6c405ce7e476a6c5ef4f87db30799ad49f765094cf9770e0f7609. Subsequent calls will reuse this data.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/3 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6f420295236246a68c7bc4186d8273f4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/200000 [00:00<?, ?ex/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5ae2ee60e5614826a8b1ed1cf644ea42"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/5000 [00:00<?, ?ex/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1753e8a5ecb9452e84f8e646307cf801"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/5000 [00:00<?, ?ex/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ad24651b1d864e81961aeba410087a5b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)okenizer_config.json:   0%|          | 0.00/360 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4381ad318e09437e9cdb2fe00b72d165"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)solve/main/vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4e977cdf5c8541a9960ad3d0dc0035b8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)/main/tokenizer.json:   0%|          | 0.00/712k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d74ffbe4d8a94b42adf2ee641b1a5378"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)cial_tokens_map.json:   0%|          | 0.00/125 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9a173225664848e29ec6cb54ea2333ac"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/200 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e2951cb4cdff4a9e8b7a631a69c2372a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/5 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"20965069cb0b495980bed4936c82f33a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/5 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0591419f4f434557962cad98c7035899"}},"metadata":{}},{"name":"stdout","text":"['labels', 'input_ids', 'attention_mask']\n","output_type":"stream"}]},{"cell_type":"code","source":"%env WANDB_PROJECT=sentiment-analysis\n%env WANDB_LOG_MODEL=end\n\nwandb.init(project=WANDB_PROJECT, job_type=\"evaluation\", tags=['staging'])","metadata":{"execution":{"iopub.status.busy":"2023-03-13T12:56:34.608007Z","iopub.execute_input":"2023-03-13T12:56:34.609087Z","iopub.status.idle":"2023-03-13T12:57:05.672262Z","shell.execute_reply.started":"2023-03-13T12:56:34.609039Z","shell.execute_reply":"2023-03-13T12:57:05.671133Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":28,"outputs":[{"name":"stdout","text":"env: WANDB_PROJECT=sentiment-analysis\nenv: WANDB_LOG_MODEL=end\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Changes to your `wandb` environment variables will be ignored because your `wandb` session has already started. For more information on how to modify your settings with `wandb.init()` arguments, please refer to <a href='https://wandb.me/wandb-init' target=\"_blank\">the W&B docs</a>."},"metadata":{}},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mamir7d0\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n","output_type":"stream"},{"name":"stdout","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"wandb version 0.13.11 is available!  To upgrade, please run:\n $ pip install wandb --upgrade"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.13.10"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20230313_125634-tpdzw6dt</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/amir7d0/sentiment-analysis/runs/tpdzw6dt' target=\"_blank\">desert-forest-20</a></strong> to <a href='https://wandb.ai/amir7d0/sentiment-analysis' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/amir7d0/sentiment-analysis' target=\"_blank\">https://wandb.ai/amir7d0/sentiment-analysis</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/amir7d0/sentiment-analysis/runs/tpdzw6dt' target=\"_blank\">https://wandb.ai/amir7d0/sentiment-analysis/runs/tpdzw6dt</a>"},"metadata":{}},{"execution_count":28,"output_type":"execute_result","data":{"text/html":"<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/amir7d0/sentiment-analysis/runs/tpdzw6dt?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>","text/plain":"<wandb.sdk.wandb_run.Run at 0x7fd2eb45ded0>"},"metadata":{}}]},{"cell_type":"markdown","source":"### Trainer","metadata":{}},{"cell_type":"code","source":"from transformers import DataCollatorWithPadding\nfrom transformers import TrainingArguments\nfrom transformers import AutoTokenizer, AutoModelForSequenceClassification\n\ndata_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n\nmodel = AutoModelForSequenceClassification.from_pretrained(MODEL_NAME)","metadata":{"execution":{"iopub.status.busy":"2023-03-13T19:20:13.403917Z","iopub.execute_input":"2023-03-13T19:20:13.404682Z","iopub.status.idle":"2023-03-13T19:20:19.635278Z","shell.execute_reply.started":"2023-03-13T19:20:13.404643Z","shell.execute_reply":"2023-03-13T19:20:19.634108Z"},"trusted":true},"execution_count":10,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading (…)lve/main/config.json:   0%|          | 0.00/827 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5dbfdad3cf994044ab2e59fd8645d956"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)\"pytorch_model.bin\";:   0%|          | 0.00/268M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"81f7a9ee188a4e198784b11d8da9f627"}},"metadata":{}}]},{"cell_type":"code","source":"from transformers import Trainer\nfrom sklearn.metrics import accuracy_score\nfrom datasets import load_metric\nimport numpy as np\n\nmetric = load_metric(\"accuracy\")\n\ndef compute_metrics(eval_pred):\n    predictions, labels = eval_pred\n    predictions = np.argmax(predictions, axis=1)\n    return metric.compute(predictions=predictions, references=labels)\n\n\ntraining_args = TrainingArguments(\"fine-tune-trainer\", \n                                  overwrite_output_dir=True,\n                                  evaluation_strategy='epoch',\n                                  save_strategy=\"epoch\",\n                                  lr_scheduler_type='linear',\n                                  learning_rate=2e-5,\n                                  num_train_epochs=5, report_to='wandb')\n\ntrainer = Trainer(\n    model,\n    training_args,\n    train_dataset=tokenized_datasets[\"train\"],\n    eval_dataset=tokenized_datasets[\"validation\"],\n    data_collator=data_collator,\n    tokenizer=tokenizer,\n    compute_metrics=compute_metrics\n)\ntrainer.train()\nwandb.finish()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"preds_dev = trainer.predict(tokenized_datasets['validation'])[0]\npreds_test = trainer.predict(tokenized_datasets['test'])[0]","metadata":{"execution":{"iopub.status.busy":"2023-03-13T19:21:18.541665Z","iopub.execute_input":"2023-03-13T19:21:18.542621Z","iopub.status.idle":"2023-03-13T19:21:58.705244Z","shell.execute_reply.started":"2023-03-13T19:21:18.542568Z","shell.execute_reply":"2023-03-13T19:21:58.704209Z"},"trusted":true},"execution_count":12,"outputs":[{"name":"stderr","text":"***** Running Prediction *****\n  Num examples = 5000\n  Batch size = 16\nYou're using a DistilBertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n/opt/conda/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"name":"stderr","text":"***** Running Prediction *****\n  Num examples = 5000\n  Batch size = 16\n/opt/conda/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}}]},{"cell_type":"code","source":"import numpy as np\nfrom sklearn.metrics import accuracy_score, top_k_accuracy_score\n\ny_true = np.array(tokenized_datasets['validation']['labels'])\ny_score = preds_dev\nprint('Accuracy (exact) on Dev Set: ', accuracy_score(y_true, y_score.argmax(1)))\n\ny_true = np.array(tokenized_datasets['test']['labels'])\ny_score = preds_test\nprint('Accuracy (exact) on Test Set: ', accuracy_score(y_true, y_score.argmax(1)))","metadata":{"execution":{"iopub.status.busy":"2023-03-13T19:23:23.990140Z","iopub.execute_input":"2023-03-13T19:23:23.990845Z","iopub.status.idle":"2023-03-13T19:23:24.006964Z","shell.execute_reply.started":"2023-03-13T19:23:23.990807Z","shell.execute_reply":"2023-03-13T19:23:24.005786Z"},"trusted":true},"execution_count":14,"outputs":[{"name":"stdout","text":"Accuracy on Dev Set:  0.5696\nAccuracy on Test Set:  0.5736\n","output_type":"stream"}]},{"cell_type":"code","source":"import numpy as np\nfrom sklearn.metrics import accuracy_score, top_k_accuracy_score\n\ny_true = np.array(tokenized_datasets['validation']['labels'])\ny_score = preds_dev\nprint('Accuracy (off-by-1) on Dev Set: ', top_k_accuracy_score(y_true, y_score, k=2))\n\ny_true = np.array(tokenized_datasets['test']['labels'])\ny_score = preds_test\nprint('Accuracy (off-by-1) on Test Set: ', top_k_accuracy_score(y_true, y_score, k=2))","metadata":{"execution":{"iopub.status.busy":"2023-03-13T19:24:53.989652Z","iopub.execute_input":"2023-03-13T19:24:53.990616Z","iopub.status.idle":"2023-03-13T19:24:54.008401Z","shell.execute_reply.started":"2023-03-13T19:24:53.990564Z","shell.execute_reply":"2023-03-13T19:24:54.007319Z"},"trusted":true},"execution_count":15,"outputs":[{"name":"stdout","text":"Accuracy (off-by-1) on Dev Set:  0.855\nAccuracy (off-by-1) on Test Set:  0.8558\n","output_type":"stream"}]},{"cell_type":"code","source":"model.push_to_hub('amir7d0/distilbert-base-uncased-finetuned-amazon-reviews')","metadata":{"execution":{"iopub.status.busy":"2023-03-13T19:26:49.918859Z","iopub.execute_input":"2023-03-13T19:26:49.919455Z","iopub.status.idle":"2023-03-13T19:26:55.641267Z","shell.execute_reply.started":"2023-03-13T19:26:49.919416Z","shell.execute_reply":"2023-03-13T19:26:55.640217Z"},"trusted":true},"execution_count":17,"outputs":[{"name":"stderr","text":"Configuration saved in /tmp/tmpyvqpfm6d/config.json\nModel weights saved in /tmp/tmpyvqpfm6d/pytorch_model.bin\nUploading the following files to amir7d0/distilbert-base-uncased-finetuned-amazon-reviews: pytorch_model.bin,config.json\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Upload 1 LFS files:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d8b8ceb2cebb4be8868b3b6f68ef32b5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"pytorch_model.bin:   0%|          | 0.00/268M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"874a754032ce441eb88fa469085f4b8e"}},"metadata":{}},{"execution_count":17,"output_type":"execute_result","data":{"text/plain":"CommitInfo(commit_url='https://huggingface.co/amir7d0/distilbert-base-uncased-finetuned-amazon-reviews/commit/a0d548c51b9f0e74a112fa3c014087169d1c53d2', commit_message='Upload DistilBertForSequenceClassification', commit_description='', oid='a0d548c51b9f0e74a112fa3c014087169d1c53d2', pr_url=None, pr_revision=None, pr_num=None)"},"metadata":{}}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 7. Deployment Phase","metadata":{"id":"1GYGj8QuC8oW"}},{"cell_type":"code","source":"## You can deploy the model in HuggingFace spaces.\n## copy src/app.py in app.py then commit and push files: \n\n# git clone https://huggingface.co/spaces/<username>/<space-name>\n\n# git add app.py\n# git commit -m \"Add application file\"\n# git push","metadata":{"id":"4j9pKs85C-EC"},"execution_count":null,"outputs":[]}]}